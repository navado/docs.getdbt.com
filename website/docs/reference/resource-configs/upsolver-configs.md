---
title: "Upsolver configurations"
id: "upsolver-configs"
description: "Upsolver Configurations - Read this in-depth guide to learn about configurations in dbt."
---

## Supported Upsolver SQLake functionality:

| COMMAND | STATE | MATERIALIZED |
| ------ | ------ | ------ |
| SQL compute cluster| not supported | - |
| SQL connections| supported | connection |
| SQL copy job | supported | incremental |
| SQL merge job | supported | incremental |
| SQL insert job | supported | incremental |
| SQL materialized views | supported | materializedview |


## Configs materialization

| Config | Required | Materialization | Description | Example |
| ------ | --------- | --------------- | ----------- | ------- |
| connection_type | Yes | connection | Connection identifier: S3/GLUE_CATALOG/KINESIS | connection_type='S3' |
| connection_options | Yes | connection | Dictionary of options supported by selected connection |           connection_options={ 'aws_role': 'aws_role', 'external_id': 'SAMPLES', 'read_only': True } |
| incremental_strategy | No | incremental | Define one of incremental strategies: merge/copy/insert. Default: copy | incremental_strategy='merge' |
| source | No | incremental | Define source to copy from: S3/KAFKA/KINESIS | source = 'S3' |
| target_type | No | incremental | Define supported target to copy into. Default: copy into a table created in a metastore connection | target_type='Snowflake' |
| target_schema | Yes/No | incremental | Define target schema. Required if target_type not table created in a metastore connection | target_schema = 'your_schema' |
| target_connection | Yes/No | incremental | Define target connection. Required if target_type not table created in a metastore connection | target_connection = 'your_snowflake_connection' |
| target_table_alias | Yes/No | incremental | Define target table. Required if target_type not table created in a metastore connection | target_table_alias = 'target_table' |
| delete_condition | No | incremental | Records that match the ON condition and a delete condition can be deleted | delete_condition='nettotal > 1000' |
| partition_by | No | incremental | List of dictionaries to define partition_by for target metastore table | partition_by=[{'field':'$field_name'}] |
| primary_key | No | incremental | List of dictionaries to define partition_by for target metastore table  | primary_key=[{'field':'customer_email', 'type':'string'}] |
| map_columns_by_name | No | incremental | Maps columns from the SELECT statement to the table. Boolean. Default: False | map_columns_by_name=True |
| sync | No | incremental/materializedview | Boolean option to define if job is synchronized or non-msynchronized. Default: False | sync=True |
| options | No | incremental/materializedview | Dictionary of job options | options={ 'START_FROM': 'BEGINNING', 'ADD_MISSING_COLUMNS': True } |


## SQL connection options

Connections are used to provide Upsolver with the proper credentials to bring your data into SQLake as well as to write out your transformed data to various services. More details on ["Upsolver SQL connections"](https://docs.upsolver.com/sqlake/sql-command-reference/sql-connections)
As a dbt model connection is a model with materialized='connection'

```sql
{{ config(
        materialized='connection',
        connection_type={ 'S3' | 'GLUE_CATALOG' | 'KINESIS' | 'KAFKA'| 'SNOWFLAKE' },
        connection_options={}
        )
}}
```

Running this model will compile CREATE CONNECTION(or ALTER CONNECTION if exists) SQL and send it to Upsolver engine. Name of the connection will be name of the model.


## SQL copy job

A COPY FROM job allows you to copy your data from a given source into a table created in a metastore connection. This table then serves as your staging table and can be used with SQLake transformation jobs to write to various target locations. More details on ["Upsolver SQL copy-from"](https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job/copy-from)

As a dbt model copy job is model with materialized='incremental'
```sql
{{ config(  materialized='incremental',
            sync=True|False,
            source = 'S3'| 'KAFKA' | ... ,
        	options={
              	'option_name': 'option_value'
            },
        	partition_by=[{}]
      	)
}}
SELECT * FROM {{ ref(<model>) }}
```
Running this model will  compile CREATE TABLE SQL(or ALTER TABLE if exists) and CREATE COPY JOB(or ALTER COPY JOB if exists) SQL and send it to Upsolver engine. Name of the table will be name of the model. Name of the job will be name of the model plus '_job'


## SQL insert job

An INSERT job defines a query that pulls in a set of data based on the given SELECT statement and inserts it into the designated target. This query is then run periodically based on the RUN_INTERVAL defined within the job. More details on ["Upsolver SQL insert"](https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job/sql-transformation-jobs/insert).

As a dbt model insert job is model with materialized='incremental' and incremental_strategy='insert'

```sql
{{ config(  materialized='incremental',
            sync=True|False,
            map_columns_by_name=True|False,
            incremental_strategy='insert',
            options={
              	'option_name': 'option_value'
            },
            primary_key=[{}]
          )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...
HAVING COUNT(DISTINCT orderid::string) ...
```

Running this model will compile CREATE TABLE SQL(or ALTER TABLE if exists) and CREATE INSERT JOB(or ALTER INSERT JOB if exists) SQL and send it to Upsolver engine. Name of the table will be name of the model. Name of the job will be name of the model plus '_job'


## SQL merge job

A MERGE job defines a query that pulls in a set of data based on the given SELECT statement and inserts into, replaces, or deletes the data from the designated target based on the job definition. This query is then run periodically based on the RUN_INTERVAL defined within the job. More details on ["Upsolver SQL merge"](https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job/sql-transformation-jobs/merge).

As a dbt model merge job is model with materialized='incremental' and incremental_strategy='merge'

```sql
{{ config(  materialized='incremental',
            sync=True|False,
            map_columns_by_name=True|False,
            incremental_strategy='merge',
            options={
              	'option_name': 'option_value'
            },
            primary_key=[{}]
          )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...
HAVING COUNT ...
```

Running this model will compile CREATE TABLE SQL(or ALTER TABLE if exists) and CREATE MERGE JOB(or ALTER MERGE JOB if exists) SQL and send it to Upsolver engine. Name of the table will be name of the model. Name of the job will be name of the model plus '_job'

## SQL materialized views

When transforming your data, you may find that you need data from multiple source tables in order to achieve your desired result.
In such a case, you can create a materialized view from one SQLake table in order to join it with your other table (which in this case is considered the main table). More details on ["Upsolver SQL materialized views"](https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job/sql-transformation-jobs/sql-materialized-views).

As a dbt model materialized views  is model with materialized='materializedview'.

```sql
{{ config(  materialized='materializedview',
            sync=True|False,
            options={'option_name': 'option_value'}
        )
}}
SELECT ...
FROM {{ ref(<model>) }}
WHERE ...
GROUP BY ...
```

Running this model will compile CREATE MATERIALIZED VIEW SQL(or ALTER MATERIALIZED VIEW if exists) and send it to Upsolver engine. Name of the materializedview  will be name of the model.

## Projects examples

> projects examples link: [github.com/dbt-upsolver/examples/](https://github.com/Upsolver/dbt-upsolver/tree/main/examples)

## Connection options

| Option | Storage    | Type | Editable | Optional | Description |
| --------| --------- | ---- | -------- | -------- | ----------- |
| aws_role | s3 | text | True | True | The AWS IAM role ARN. Used in conjunction with EXTERNAL_ID. If omitted, the role created when integrating Upsolver with the AWS account is used. To learn how to provide a role with the proper credentials, see: Configure access to S3 |
| external_id | s3 | text | True | True | The external ID of the role to assume. Used in conjunction with AWS_ROLE. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_access_key_id | s3 | text | True | True | The AWS access key ID. Used in conjunction with AWS_SECRET_ACCESS_KEY. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_secret_access_key_id | s3 | text | True | True | The AWS secret key corresponding to the provided AWS_ACCESS_KEY_ID. If omitted, the role created when integrating Upsolver with the AWS account is used |
| path_display_filter | s3 | text | True | True | A single path to show. If not provided, all buckets are shown. Paths should be provided in the following format: s3://bucket/prefix. This shows anything beginning with the given prefix. To filter by a specific folder, use the following format: s3://bucket/folder-path/ |
| path_display_filters | s3 | list | True | True | The list of paths to show. If not provided, all buckets are shown. Paths should be provided in the following format: s3://bucket/prefix. This shows anything beginning with the given prefix. To filter by a specific folder, use the following format: s3://bucket/folder-path/ |
| read_only | s3 | boolean | True | True | Whether or not the connection is read-only. When true, Upsolver is not able to write data to or delete data from, the bucket |
| encryption_kms_key | s3 | text | True | True | The ARN of the KMS key to use. If omitted, uses the default encryption defined on the bucket in AWS |
| encryption_customer_kms_key | s3 | text | True | True | The Base64 text representation of the encryption key to use. If omitted, uses the default encryption defined on the bucket in AWS |
| comment | s3 | text | True | True | A description or comment regarding this connection |
| host | kafka | text | False | False | A single host in the format of hostname:port |
| hosts | kafka | list | False | False | The list of Kafka hosts in the format of hostname:port |
| consumer_properties | kafka | text | True | True | Extra properties to configure for the consumer |
| version | kafka | value | False | True | Legacy is for anything before 0.10. Default: CURRENT |
| require_static_ip | kafka | boolean | True | True | With Upsolver clusters, you can configure how many elastic IPs it should allocate and use within that cluster. If the cluster running the job has at least one elastic IP set and REQUIRE_STATIC_IP is enabled, then the job runs on a server that has an elastic IP associated with it. Default: true |
| ssl | kafka | boolean | True | True | If enabled, SSL is used to connect. Please contact Upsolver to ensure your CA certificate is supported. Default: false |
| topic_display_filter | kafka | text | True | True | A single topic to show. If left empty, all topics are visible |
| topic_display_filters | kafka | list | True | True | The list of topics to show. If left empty, all topics are visible |
| comment | kafka | text | True | True | A description or comment regarding this connection |
| aws_role | glue_catalog | text | True | True | The AWS IAM role ARN. Used in conjunction with EXTERNAL_ID. If omitted, the role created when integrating Upsolver with the AWS account is used |
| external_id | glue_catalog | text | True | True | The external ID of the role to assume. Used in conjunction with AWS_ROLE. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_access_key_id | glue_catalog | text | True | True | The AWS access key ID. Used in conjunction with AWS_SECRET_ACCESS_KEY. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_secret_access_key | glue_catalog | text | True | True | The AWS secret key corresponding to the provided AWS_ACCESS_KEY_ID. If omitted, the role created when integrating Upsolver with the AWS account is used |
| default_storage_connection | glue_catalog | identifier | False | False | An Amazon S3 connection with the appropriate credentials to write to the DEFAULT_STORAGE_LOCATION provided |
| default_storage_location | glue_catalog | text | False | False | The Amazon S3 path that serves as the default storage location for the underlying files associated with tables created under this metastore connection |
| region | glue_catalog | text | False | True | The region your Glue Catalog is in. Default: Region in which Upsolver is deployed within your AWS account |
| database_display_filter | glue_catalog | text | True | True | A single database to show. If left empty, all databases are visible |
| database_display_filters | glue_catalog | list | True | True | The list of databases to show. If left empty, all databases are visible |
| comment | glue_catalog | text | True | True | A description or comment regarding this connection |
| aws_role | kinesis | text | True | True | The AWS IAM role ARN. Used in conjunction with EXTERNAL_ID. If omitted, the role created when integrating Upsolver with the AWS account is used |
| external_id | kinesis | text | True | True | The external ID of the role to assume. Used in conjunction with AWS_ROLE. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_access_key_id | kinesis | text | True | True | The AWS access key ID. Used in conjunction with AWS_SECRET_ACCESS_KEY. If omitted, the role created when integrating Upsolver with the AWS account is used |
| aws_secret_access_key | kinesis | text | True | True | The AWS secret key corresponding to the provided AWS_ACCESS_KEY_ID. If omitted, the role created when integrating Upsolver with the AWS account is used |
| region | kinesis | text | False | False | The AWS region to use |
| read_only | kinesis | boolean | False | True | When true, the connection can only be used to read data from Kinesis and not for writing data to Kinesis. Default: false |
| max_writers | kinesis | integer | True | True | The number of maximum parallel writers to Kinesis. Default: 20 |
| stream_display_filter | kinesis | text | True | True | A single stream to show. If left empty, all streams are visible |
| stream_display_filters | kinesis | list | True | True | The list of streams to show. If left empty, all streams are visible |
| comment | kinesis | text | True | True | A description or comment regarding this connection |
| connection_string | snowflake | text | True | False | The connection string to use when connecting to the database. Format: jdbc:snowflake://<ACCOUNT_WITH_REGION>.snowflakecomputing.com?db=<DB_NAME>&warehouse=<WAREHOUSE_NAME>&role=<ROLE_NAME > Where: ACCOUNT_WITH_REGION.snowflakecomputing.com The connection URL in Snowflake. Example: snowflakedemo.us-east-2.aws.snowflakecomputing.com DB_NAME The name of the database to connect to. WAREHOUSE_NAME (Optional) The warehouse name. If not provided, the default warehouse is used. If no default warehouse exists, the CREATE CONNECTION command fails. ROLE_NAME (Optional) The name of the role to use when connecting. If not provided, the default role is used. If no default role exists, the CREATE CONNECTION command fails. Read more about connection string arguments in Snowflake |
| user_name | snowflake | text | True | False | The user to authenticate to the database with |
| password | snowflake | text | True | False | The password for the user |
| max_concurrent_connections | snowflake | integer | True | True | The maximum number of concurrent connections to the database. Limiting this may reduce the load on the database but could result in longer data latency |
| comment | snowflake | text | True | True | A description or comment regarding this connection |
| connection_string | redshift | text | True | False | The connection string to use when connecting to the database. Format: jdbc:snowflake://<ACCOUNT_WITH_REGION>.snowflakecomputing.com?db=<DB_NAME>&warehouse=<WAREHOUSE_NAME>&role=<ROLE_NAME > Where: ACCOUNT_WITH_REGION.snowflakecomputing.com The connection URL in Snowflake. Example: snowflakedemo.us-east-2.aws.snowflakecomputing.com DB_NAME The name of the database to connect to. WAREHOUSE_NAME (Optional) The warehouse name. If not provided, the default warehouse is used. If no default warehouse exists, the CREATE CONNECTION command fails. ROLE_NAME (Optional) The name of the role to use when connecting. If not provided, the default role is used. If no default role exists, the CREATE CONNECTION command fails. Read more about connection string arguments in Snowflake |
| user_name | redshift | text | True | False | The user to authenticate to the database with |
| password | redshift | text | True | False | The password for the user |
| max_concurrent_connections | redshift | integer | True | True | The maximum number of concurrent connections to the database. Limiting this may reduce the load on the database but could result in longer data latency |
| comment | redshift | text | True | True | A description or comment regarding this connection |
| connection_string | mysql | text | True | False | The connection string to use when connecting to the database. This string can be copied directly from MySQL. The connection string should include the database name at the end of the string |
| user_name | mysql | text | True | False | The user to authenticate to the database with |
| password | mysql | text | True | False | The password for the user |
| comment | mysql | text | True | True | A description or comment regarding this connection |
| connection_string | postgres | text | True | False | The connection string to use when connecting to the database. This string can be copied directly from PostgreSQL |
| user_name | postgres | text | True | False | The user to authenticate to the database with. |
| password | postgres | text | True | False | The password for the user. |
| comment | postgres | text | True | True | A description or comment regarding this connection. |
| connection_string | elasticsearch | text | True | False | The connection string to use when connecting to the cluster. Format: 'elasticsearch://host:port?cluster.name=your_cluster_name' |
| user_name | elasticsearch | text | True | False | The user to authenticate to the cluster. |
| password | elasticsearch | text | True | False | The password for the user. |
| comment | elasticsearch | text | True | True | A description or comment regarding this connection. |

## Target options

| Option | Storage    | Type | Editable | Optional | Description |
| --------| --------- | ---- | -------- | -------- | ----------- |
| globally_unique_keys | datalake | boolean | False | True | By default, partition keys are implicitly part of the primary key. This means that when upserting, only rows with the same primary key and partition are replaced. This is the more performant and therefore recommended option. However, some use cases may require that the primary keys be globally unique (in other words, unique across partitions). This means that when upserting, rows with the same primary key should be replaced, even if they belong to different partitions. Note that this also means that rows can "switch" partitions. If such is the case, you should set this option as true. Default: false |
| storage_connection | datalake | identifier | False | True | The storage connection associated with the STORAGE_LOCATION for the table's underlying files. Only a storage type connection can be used here (e.g. S3, Blob storage, GCS, Oracle object storage), and it should match the catalog's metastore. For example, if Glue is used as the metastore, only S3 is allowed as a storage connection. When set, STORAGE_LOCATION must be configured as well to provide a path to store the data. Default: Default storage connection configured for the metastore connection this table is created under |
| storage_location | datalake | text | False | True | The storage location for the table's underlying files. For S3, it should be provided in the format s3://bucket_name/path_to_data. This option is required when STORAGE_CONNECTION is set. When set, STORAGE_CONNECTION must be configured as well to provide a connection with access to write to the specified storage location. Default: Default storage location configured for the metastore connection this table is created under |
| compute_cluster | datalake | identifier | True | True | The compute cluster that processes the table. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| compression | datalake | value | True | True | Type of compression for the table data. Values: { SNAPPY | GZIP } Default: SNAPPY |
| compaction_processes | datalake | integer | True | True | This determines the number of compaction processes your table can do in parallel when it periodically compacts your data. Default: 1 |
| disable_compaction | datalake | boolean | True | True | When true, disables the compaction process. Default: false |
| retention_date_partition | datalake | identifier | False | True | This configures the partition column to use to determine whether the retention period has passed for a given record. This option is required if you have more than one date partition column. Default: The only partition column of type date |
| table_data_retention | datalake | integer | True | True | When set, data in partitions that have passed the retention period are deleted from the table. Number of days can range between 1 and 9999. This option is not a deterministic mechanism that deletes data when it immediately surpasses the defined threshold. This mechanism is closer to the lifecycle policies on common blob storage services, such as Amazon S3, and is designed to save storage costs, not to delete data based on a specific time. Therefore when data passes the retention period, it will be deleted at some point in the future, and can no longer be relied to exist, though Upsolver aims to delete it within a reasonable timeframe. You should be aware that transformation job that reads from a table with a defined data retention may or may not read data that has surpassed the retention threshold. For example, if the current time is 2023-02-23 12:30:00 UTC, and you have defined TABLE_DATA_RETENTION = 2 days, you can expect data written during 2023-02-23, 2023-02-22, and 2023-02-21 to exist in the table. The retention threshold truncates data to the nearest day, so when the time changes to 2023-02-24 00:00:00 UTC, you can no longer expect data from 2023-02-21 to be present in the table, although it might be there for a while. Note that you need at least one date partition column for this option to work. Value: `<integer>` DAYS |
| column_data_retention | datalake | list | True | True | When set, after the duration of a column elapses in a partition, the data is rewritten without the contents of that column. Number of days can range between 1 and 9999. Note that you need at least one date partition column for this to work. Type: list of (<column_name>, `<integer>` DAYS) pairs |
| comment | datalake | text | True | True | A description or comment regarding this table. |
| storage_connection | materialized_view | identifier | False | True | The storage connection associated with the STORAGE_LOCATION for the table's underlying files. Only a storage type connection can be used here (e.g. S3, Blob storage, GCS, Oracle object storage), and it should match the catalog's metastore. For example, if Glue is used as the metastore, only S3 is allowed as a storage connection. When set, STORAGE_LOCATION must be configured as well to provide a path to store the data. Default: The storage connection of the first table in the FROM statement |
| storage_location | materialized_view | text | False | True | The storage location for the materialized view's underlying files. It should be provided in the format s3://bucket_name/path_to_data. This option is required when STORAGE_CONNECTION is set. When set, STORAGE_CONNECTION must be configured as well to provide a connection with access to write to the specified storage location. Default: The storage location of the first table in the FROM statement |
| max_time_travel_duration | materialized_view | integer | True | True | How long, in days, the state information maintained by the materialized view should be retained. By default, the state is maintained indefinitely, allowing you to time travel to any point in time from the creation of the MV. Default: infinite |
| compute_cluster | materialized_view | identifier | True | True | The compute cluster that processes the materialized view. Default: The compute cluster of the first source table within the SELECT statement |
| column_transformations | snowflake | dict | False | True | If transformations must be applied prior to data landing in your target, you can use this option to perform data transformations during ingestion. When ingesting into the data lake, it is recommended that you only apply essential transformations, such as protecting PII, as it is easier to make amendments or corrections at a later date if the data remains in its raw state and instead use a transformation job to apply modifications. Therefore, as a general rule, you should only transform data that must be modified before it reaches the target. However, transformations provide the flexibility to shape your data before it lands in the target. You can use all the functions and operators supported by Upsolver to create calculated fields within your ingestion job. New columns can be added to your target, and existing column data can be transformed. You can perform actions such as converting data types, formatting string values, and concatenating columns to create a new column. If you need to mask sensitive or personally identifiable information (PII) prior to loading into your staging tables or when performing direct ingestion into your target destination, you can use hashing functions to prevent data from being exposed downstream. Combining hash functions with the EXCLUDE_COLUMNS option enables you to control your data protection. Values: ( `<column>` = `<expression>`, ...) |
| deduplicate_with | snowflake | dict | False | True | You can use DEDUPLICATE_WITH to prevent duplicate rows arriving in your target. One or more columns can be supplied in the column list to act as a key so that all events within the timeframe specified in the WINDOW value are deduplicated. For example, if you have a third-party application that sends the same event multiple times a day, you can define one or more columns as the key and set the timeframe to be 1 DAY. Upsolver will exclude all duplicate events that arrive within the day, ensuring your target only receives unique events. Note that if you have multiple jobs writing to a table in your lake, duplicate rows can be generated, even when you include this option. Values: ( {COLUMNS = (, ...) | COLUMN = }, WINDOW = { MINUTE[S] | HOUR[S] | DAY[S] } ) |
| exclude_columns | snowflake | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. Values: ( `<column>`, ...) |
| create_table_if_missing | snowflake | boolean | False | True |  |
| write_interval | snowflake | integer | False | True |  |

## Transformation options

| Option | Storage    | Type | Editable | Optional | Description |
| --------| --------- | ---- | -------- | -------- | ----------- |
| run_interval | s3 | ineger | False | True | How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5.RUN_INTERVAL Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 1 MINUTE (Optional) How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } |
| start_from | s3 | value | False | True | Configures the time to start inserting data from. Data before the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set a start time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW or BEGINNING, the job runs from the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes starting from NOW means that the first task executed by the job starts from 12:00 PM. Values: { NOW | BEGINNING | timestamp } Default: BEGINNING |
| end_at | s3 | value | True | True | Configures the time to stop inserting data. Data after the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set an end time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW, the job runs up until the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes ending at NOW means that the last task executed by the job ends at 12:00 PM. Values: { NOW | timestamp } Default: Never |
| compute_cluster | s3 | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| comment | s3 | text | True | True | A description or comment regarding this job. |
| allow_cartesian_products | s3 | boolean | False | True | When true, flattening unrelated arrays may lead to Cartesian products in your final result. See: UNNEST Default: false |
| aggregation_parallelism | s3 | integer | True | True | Only supported when the query contains aggregations. Formally known as "output sharding." Default: 1 |
| run_parallelism | s3 | integer | True | True | Controls how many jobs run in parallel to process a single minute of data from the source table. Increasing this can lower the end-to-end latency if you have lots of data per minute. Default: 1 |
| file_format | s3 | value | False | False | The file format for the output file. Values: { CSV | TSV | AVRO | PARQUET | JSON } |
| compression | s3 | value | False | True | The compression for the output files. Values: { NONE | GZIP | SNAPPY | ZSTD } Default: NONE |
| date_pattern | s3 | text | False | True | Upsolver uses the date pattern to partition the output on the S3 bucket. Upsolver supports partitioning up to the minute, for example: 'yyyy/MM/dd/HH/mm'. For more options, see: Java SimpleDateFormat Default: 'yyyy/MM/dd/HH/mm' |
| output_offset | s3 | integer | False | True | By default, the file 2023/01/01/00/01 contains data for 2023-01-01 00:00 - 2023-01-01 00:00.59.999. Setting OUTPUT_OFFSET to 1 MINUTE add to that so a value of the first minute will move the file name to 02, if you want to move it back you can use negative values. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 0 |
| location | s3 | text | False | False | The target location to write files to, as a full S3 URI. The location URI pattern can include macros referring to data columns, this allows custom partitioning of the data in the target location. Supported macros: Time: {time:`<date-pattern>`} This macro will be replaced with the job execution time at runtime. The date pattern provided must be in Java's date formatting syntax. Only a single-time macro can be used in the location. Column: {col:`<column-name>`} This macro will be replaced with the value of the column provided. The column provided must appear in the select statement of the job. Shard: {shard:format} This macro will be replaced by the output shard number writing the current file. It is important to use this as part of your pattern if you are using RUN_PARALLELISM, otherwise, each shard will overwrite the file. The supported format is a subset of Java's string fromat syntax. The supported options are either: 1. %0xd - Will result in a shard number padded with x-1 leading 0's. For example, %05d will result in 00001 for shard number 1. 2. %d - Will simply use the shard number with no padding. Usually, it's recommended to include padding to ensure alphabetical sorting of the output files. |
| run_interval | elasticsearch | ineger | False | True | How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5.RUN_INTERVAL Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 1 MINUTE (Optional) How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } |
| start_from | elasticsearch | value | False | True | Configures the time to start inserting data from. Data before the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set a start time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW or BEGINNING, the job runs from the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes starting from NOW means that the first task executed by the job starts from 12:00 PM. Values: { NOW | BEGINNING | timestamp } Default: BEGINNING |
| end_at | elasticsearch | value | True | True | Configures the time to stop inserting data. Data after the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set an end time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW, the job runs up until the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes ending at NOW means that the last task executed by the job ends at 12:00 PM. Values: { NOW | timestamp } Default: Never |
| compute_cluster | elasticsearch | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| allow_cartesian_products | elasticsearch | boolean | False | True | When true, flattening unrelated arrays may lead to Cartesian products in your final result. See: UNNEST Default: false |
| aggregation_parallelism | elasticsearch | integer | True | True | Only supported when the query contains aggregations. Formally known as "output sharding." Default: 1 |
| run_parallelism | elasticsearch | integer | True | True | Controls how many jobs run in parallel to process a single minute of data from the source table. Increasing this can lower the end-to-end latency if you have lots of data per minute. Default: 1 |
| bulk_max_size_bytes | elasticsearch | integer | True | True | The max size of each bulk insert into the index. This option defaults to 9MB. Default: 9 |
| index_partition_size | elasticsearch | value | True | True | The size of each partition of the index. The default is value is DAILY. Values: { HOURLY | DAILY | MONTHLY | YEARLY } Default: DAILY |
| comment | elasticsearch | text | True | True | A description or comment regarding this job. |
| custom_insert_expressions | snowflake | list | True | True | Configure a list of custom expression transformations to apply to the value of each column when inserting unmatched (new) rows. Note this is only used in Merge Jobs. Note: You can use {} as a placeholder for the mapped value from the select statement. Type: array[(column, expression)] Default: () |
| custom_update_expressions | snowflake | list | True | True | Configure a list of custom expression transformations to apply to the value of each column when updating matched rows. Note this is only used in Merge Jobs. Note: You can use {} as a placeholder for the mapped value from the select statement. Type: array[(column, expression)] Default: () |
| keep_existing_values_when_null | snowflake | boolean | True | True | If enabled, updates to the table preserve the previous non-null value. This option is useful if your update events only contain values for modified columns. This works by coalescing the new value with the existing value. If the new value is null the previous value will be preserved. This means that updating values to null is not supported. Default: false. |
| add_missing_columns | snowflake | boolean | False | True |  |
| run_interval | snowflake | ineger | False | True | How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. RUN_INTERVAL Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 1 MINUTE (Optional) How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } |
| start_from | snowflake | value | False | True | Configures the time to start inserting data from. Data before the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set a start time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW or BEGINNING, the job runs from the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes starting from NOW means that the first task executed by the job starts from 12:00 PM. Values: { NOW | BEGINNING | timestamp } Default: BEGINNING |
| end_at | snowflake | value | True | True | Configures the time to stop inserting data. Data after the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set an end time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW, the job runs up until the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes ending at NOW means that the last task executed by the job ends at 12:00 PM. Values: { NOW | timestamp } Default: Never |
| compute_cluster | snowflake | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| allow_cartesian_products | snowflake | boolean | False | True | When true, flattening unrelated arrays may lead to Cartesian products in your final result. See: UNNEST Default: false |
| aggregation_parallelism | snowflake | integer | True | True | Only supported when the query contains aggregations. Formally known as "output sharding." Default: 1 |
| run_parallelism | snowflake | integer | True | True | Controls how many jobs run in parallel to process a single minute of data from the source table. Increasing this can lower the end-to-end latency if you have lots of data per minute. Default: 1 |
| comment | snowflake | text | True | True | A description or comment regarding this job. |
| add_missing_columns | datalake | boolean | False | True | When true, columns that don't exist in the target table are added automatically when encountered. When false, you cannot do SELECT * within the SELECT statement of your transformation job. Default: false |
| run_interval | datalake | ineger | False | True | How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5.RUN_INTERVAL Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 1 MINUTE (Optional) How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } |
| start_from | datalake | value | False | True | Configures the time to start inserting data from. Data before the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set a start time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW or BEGINNING, the job runs from the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes starting from NOW means that the first task executed by the job starts from 12:00 PM. Values: { NOW | BEGINNING | timestamp } Default: BEGINNING |
| end_at | datalake | value | True | True | Configures the time to stop inserting data. Data after the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set an end time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW, the job runs up until the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes ending at NOW means that the last task executed by the job ends at 12:00 PM. Values: { NOW | timestamp } Default: Never |
| compute_cluster | datalake | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| allow_cartesian_products | datalake | boolean | False | True | When true, flattening unrelated arrays may lead to Cartesian products in your final result. See: UNNEST Default: false |
| aggregation_parallelism | datalake | integer | True | True | Only supported when the query contains aggregations. Formally known as "output sharding." Default: 1 |
| run_parallelism | datalake | integer | True | True | Controls how many jobs run in parallel to process a single minute of data from the source table. Increasing this can lower the end-to-end latency if you have lots of data per minute. Default: 1 |
| comment | datalake | text | True | True | A description or comment regarding this job. |
| run_interval | redshift | ineger | False | True | How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5.RUN_INTERVAL Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } Default: 1 MINUTE (Optional) How often the job runs. The runs take place over a set period of time defined by this interval and they must be divisible by the number of hours in a day. For example, you can set RUN_INTERVAL to 2 hours (the job runs 12 times per day), but trying to set RUN_INTERVAL to 5 hours would fail since 24 hours is not evenly divisible by 5. Value: `<integer>` { MINUTE[S] | HOUR[S] | DAY[S] } |
| start_from | redshift | value | False | True | Configures the time to start inserting data from. Data before the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set a start time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW or BEGINNING, the job runs from the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes starting from NOW means that the first task executed by the job starts from 12:00 PM. Values: { NOW | BEGINNING | timestamp } Default: BEGINNING |
| end_at | redshift | value | True | True | Configures the time to stop inserting data. Data after the specified time is ignored. If set as a timestamp, it should be aligned to the RUN_INTERVAL. For example, if RUN_INTERVAL is set to 5 minutes, then you can set an end time of 12:05 PM but not 12:03 PM. Additionally, the timestamp should be based in UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS'. If set to NOW, the job runs up until the previous full period. For example, if the current time is 12:03 PM, creating the job with a RUN_INTERVAL of 5 minutes ending at NOW means that the last task executed by the job ends at 12:00 PM. Values: { NOW | timestamp } Default: Never |
| compute_cluster | redshift | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is just one cluster in your environment. Once you have more than one compute cluster, you are required to provide which one to use through this option. Default: The sole cluster in your environment |
| allow_cartesian_products | redshift | boolean | False | True | When true, flattening unrelated arrays may lead to Cartesian products in your final result. See: UNNEST Default: false |
| aggregation_parallelism | redshift | integer | True | True | Only supported when the query contains aggregations. Formally known as "output sharding." Default: 1 |
| run_parallelism | redshift | integer | True | True | Controls how many jobs run in parallel to process a single minute of data from the source table. Increasing this can lower the end-to-end latency if you have lots of data per minute. Default: 1 |
| skip_failed_files | redshift | boolean | False | True | When true, the job will skip any files in which the job is unsuccessful and continue with the rest of the files. Default: true |
| fail_on_write_error | redshift | boolean | False | True | When true, the job will fail when an on-write error occurs. Default: false |
| comment | redshift | text | True | True | A description or comment regarding this job. |

## Copy options

| Option | Storage    | Category | Type | Editable | Optional | Description |
| -------| ---------- | -------- | -----| -------- | -------- | ----------- |
| topic | kafka | source_options | text | False | False | The topic to read from. |
| exclude_columns | kafka | job_options | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. |
| deduplicate_with | kafka | job_options | dict | False | True | You can use DEDUPLICATE_WITH to prevent duplicate rows arriving in your target. One or more columns can be supplied in the column list to act as a key so that all events within the timeframe specified in the WINDOW value are deduplicated. For example, if you have a third-party application that sends the same event multiple times a day, you can define one or more columns as the key and set the timeframe to be 1 DAY. Upsolver will exclude all duplicate events that arrive within the day, ensuring your target only receives unique events. Note that if you have multiple jobs writing to a table in your lake, duplicate rows can be generated, even when you include this option. |
| consumer_properties | kafka | job_options | text | True | True | Additional properties to use when configuring the consumer. This overrides any settings in the Apache Kafka connection. |
| reader_shards | kafka | job_options | integer | True | True | Determines how many readers are used in parallel to read the stream. This number does not need to equal your number of partitions in Apache Kafka. A recommended value would be to increase it by 1 for every 70 MB/s sent to your topic. Default: 1 |
| store_raw_data | kafka | job_options | boolean | False | True | When true, an additional copy of the data is stored in its original format. Default: false |
| start_from | kafka | job_options | value | False | True | Configures the time from which to start ingesting data. Files before the specified time are ignored. Default: BEGINNING Values: { NOW | BEGINNING } |
| end_at | kafka | job_options | value | True | True | Configures the time to stop ingesting data. Files after the specified time are ignored. Timestamps should be based on UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS' Values: { NOW | <timestamp> } Default: Never |
| compute_cluster | kafka | job_options | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is only one cluster in your environment. If you have more than one compute cluster, you need to determine which one to use through this option. Default: The sole cluster in your environment |
| run_parallelism | kafka | job_options | integer | True | True | The number of parser jobs to run in parallel per minute. Default: 1 |
| content_type | kafka | job_options | value | True | True | The file format of the content being read. Note that AUTO only works when reading Avro, JSON, or Parquet. To configure additional options for particular content types, see Content type options. Values: { AUTO | CSV | JSON | PARQUET | TSV | AVRO | AVRO_SCHEMA_REGISTRY | FIXED_WIDTH | REGEX | SPLIT_LINES | ORC | XML } Default: AUTO |
| compression | kafka | job_options | value | False | True | The compression format of the source. Values: { AUTO | GZIP | SNAPPY | LZO | NONE | SNAPPY_UNFRAMED | KCL } Default: AUTO |
| comment | kafka | job_options | text | True | True | A description or comment regarding this job. |
| table_include_list | mysql | source_options | list | True | True | Comma-separated list of regular expressions that match fully-qualified table identifiers of tables whose changes you want to capture. This maps to the Debezium table.include.list property. By default, the connector captures changes in every non-system table in all databases. To match the name of a table, SQLake applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table. It does not match substrings that might be present in a table name. Default: '' |
| column_exclude_list | mysql | source_options | list | True | True | Comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. This maps to Debezium column.exclude.list property. By default, the connector matches all columns of the tables listed in TABLE_INCLUDE_LIST. To match the name of a column, SQLake applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name. Default: '' |
| exclude_columns | mysql | job_options | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. |
| column_transformations | mysql | job_options | dict | False | True | If transformations must be applied prior to data landing in your target, you can use this option to perform data transformations during ingestion. When ingesting into the data lake, it is recommended that you only apply essential transformations, such as protecting PII, as it is easier to make amendments or corrections at a later date if the data remains in its raw state and instead use a transformation job to apply modifications. Therefore, as a general rule, you should only transform data that must be modified before it reaches the target. However, transformations provide the flexibility to shape your data before it lands in the target. You can use all the functions and operators supported by Upsolver to create calculated fields within your ingestion job. New columns can be added to your target, and existing column data can be transformed. You can perform actions such as converting data types, formatting string values, and concatenating columns to create a new column. If you need to mask sensitive or personally identifiable information (PII) prior to loading into your staging tables or when performing direct ingestion into your target destination, you can use hashing functions to prevent data from being exposed downstream. Combining hash functions with the EXCLUDE_COLUMNS option enables you to control your data protection. |
| skip_snapshots | mysql | job_options | boolean | True | True | By default, snapshots are enabled for new tables. This means that SQLake will take a full snapshot of the table(s) and ingest it into the staging table before it continues to listen for change events. When set to True, SQLake will not take an initial snapshot and only process change events starting from the time the ingestion job is created. In the majority of cases, when you connect to your source tables, you want to take a full snapshot and ingest it as the baseline of your table. This creates a full copy of the source table in your data lake before you begin to stream the most recent change events. If you skip taking a snapshot, you will not have the historical data in the target table, only the newly added or changed rows. Skipping a snapshot is useful in scenarios where your primary database instance crashed or became unreachable, failing over to the secondary. In this case, you will need to re-establish the CDC connection but would not want to take a full snapshot because you already have all of the history in your table. In this case, you would want to restart processing from the moment you left off when the connection to the primary database went down. |
| end_at | mysql | job_options | value | True | True | Configures the time to stop ingesting data. Files after the specified time are ignored. Timestamps should be based on UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS' Default: Never |
| compute_cluster | mysql | job_options | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is only one cluster in your environment. If you have more than one compute cluster, you need to determine which one to use through this option. Default: The sole cluster in your environment |
| comment | mysql | job_options | text | True | True | A description or comment regarding this job. |
| table_include_list | postgres | source_options | list | False | False | Comma-separated list of regular expressions that match fully-qualified table identifiers of tables whose changes you want to capture. Tables not included in this list will not be loaded. If the list is left empty all tables will be loaded. This maps to Debezium table.include.list property. By default, the connector captures changes in every non-system table in all databases. To match the name of a table, SQLake applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table. It does not match substrings that might be present in a table name. Default: '' |
| column_exclude_list | postgres | source_options | list | False | True | Comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. This maps to the Debezium column.exclude.list property. By default, the connector matches all columns of the tables listed in TABLE_INCLUDE_LIST. To match the name of a column, SQLake applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name. Default: '' |
| heartbeat_table | postgres | job_options | text | False | True | If it is not set, no heartbeat table is used. Using a heartbeat table is recommended to avoid the replication slot growing indefinitely when no CDC events are captured for the subscribed tables. |
| skip_snapshots | postgres | job_options | boolean | False | True | The snapshot-taking process will be skipped. This is a way to skip the original snapshot-taking process for this specific job. The remainder of the job actions will be the same. Default: false |
| publication_name | postgres | job_options | text | False | False | Adds a new publication to the current database. The publication name must be distinct from the name of any existing publication in the current database. DDL will be filtered. |
| end_at | postgres | job_options | value | True | True | Configures the time to stop ingesting data. Files after the specified time are ignored. Timestamps should be based on UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS' Values: { NOW | <timestamp> } Default: Never |
| start_from | postgres | job_options | value | False | True | (Optional) Configures the time to start ingesting data from. Files before the specified time are ignored. Timestamps provided should be based on UTC. When a DATE_PATTERN is not specified, configuring this option is not allowed. By default, all data available is ingested. If theDATE_PATTERN is not lexicographically ordered, then this option cannot be set to BEGINNING. Values: { NOW | BEGINNING | <timestamp> } Default: BEGINNING |
| compute_cluster | postgres | job_options | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is only one cluster in your environment. If you have more than one compute cluster, you need to determine which one to use through this option. Default: The sole cluster in your environment |
| comment | postgres | job_options | text | True | True | A description or comment regarding this job. |
| parse_json_columns | postgres | job_options | boolean | False | False | If enabled, Upsolver will parse JSON columns into a struct matching the JSON value. Default: false |
| column_transformations | postgres | job_options | dict | False | True | If transformations must be applied prior to data landing in your target, you can use this option to perform data transformations during ingestion. When ingesting into the data lake, it is recommended that you only apply essential transformations, such as protecting PII, as it is easier to make amendments or corrections at a later date if the data remains in its raw state and instead use a transformation job to apply modifications. Therefore, as a general rule, you should only transform data that must be modified before it reaches the target. However, transformations provide the flexibility to shape your data before it lands in the target. You can use all the functions and operators supported by Upsolver to create calculated fields within your ingestion job. New columns can be added to your target, and existing column data can be transformed. You can perform actions such as converting data types, formatting string values, and concatenating columns to create a new column. If you need to mask sensitive or personally identifiable information (PII) prior to loading into your staging tables or when performing direct ingestion into your target destination, you can use hashing functions to prevent data from being exposed downstream. Combining hash functions with the EXCLUDE_COLUMNS option enables you to control your data protection. |
| exclude_columns | postgres | job_options | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. |
| location | s3 | source_options | text | False | False | The location to read files from, as a full Amazon S3 URI. |
| date_pattern | s3 | job_options | text | False | True | The date pattern of the partitions on the Amazon S3 bucket to read from. SQLake supports reading from buckets partitioned up to the minute. Example: 'yyyy/MM/dd/HH/mm'. When you set a DATE_PATTERN, SQLake uses the date in the folder path to understand when new files are added. The date in the path is used to process data in order of arrival, as well as set the $source_time and $event_time system columns used to keep jobs synchronized. If files are added to a folder named with a future date, these files will not be ingested until that date becomes the present. If you don’t set a DATE_PATTERN, SQLake will list and ingest files in the ingest job’s BUCKET and PREFIX location as soon as they are discovered. Historical data will also be processed as soon as it is added and discovered by SQLake. To discover new files, when a DATE_PATTERN is not set, SQLake lists the top-level prefix and performs a diff to detect newly added files. Subsequently, it lists the paths adjacent to these new files with the assumption that if a file was added here, others will be as well. This process is performed at regular intervals to ensure files are not missed. For buckets with few files and predictable changes, this works well. However, for buckets with many changes across millions of files and hundreds of prefixes, the scanning and diffing process may result in ingestion and processing delay. To optimize this process, consider setting the job’s DELETE_FILES_AFTER_LOAD property to TRUE. This moves ingested files to another staging location, leaving the source folder empty, and making it easier and faster for SQLake to discover new files. Be aware that configuring SQLake to move ingested files could impact other systems if they depend on the same raw files. To troubleshoot jobs that ingest data, you can query the task execution system table and inspect whether 0 bytes of data have been read in the “ingest data” stage, or SQLake is throwing parse errors in the “parse data” stage. In the case that 0 bytes have been read, it means that your job is configured correctly, but there is no new data. In the case where you see parse errors, you can narrow it down to either a misconfiguration of the job or bad data. |
| file_pattern | s3 | job_options | text | False | True | Only files that match the provided regex pattern are loaded. Use this option to filter out irrelevant data. For example, you could filter by a suffix to only keep .parquet files in a folder that may have some additional files that should not be ingested. Default: '' |
| initial_load_pattern | s3 | job_options | text | False | True | Any file matching this regex pattern is immediately loaded when the job is run. This loads data separately from the date pattern and is primarily used in CDC use cases, where you load some initial files named LOAD00001, LOAD00002, etc. After that, all the data has a date pattern in the file name. |
| initial_load_prefix | s3 | job_options | text | False | True | Any file matching this prefix is immediately loaded when the job is run. |
| delete_files_after_load | s3 | job_options | boolean | False | True | When true, files are deleted from the storage source once they have been ingested into the target location within your metastore. This allows Upsolver to discover new files immediately, regardless of how many files are in the source, or what file names and patterns are used. Default: false |
| deduplicate_with | s3 | job_options | boolean | False | True | You can use DEDUPLICATE_WITH to prevent duplicate rows arriving in your target. One or more columns can be supplied in the column list to act as a key so that all events within the timeframe specified in the WINDOW value are deduplicated. For example, if you have a third-party application that sends the same event multiple times a day, you can define one or more columns as the key and set the timeframe to be 1 DAY. Upsolver will exclude all duplicate events that arrive within the day, ensuring your target only receives unique events. Note that if you have multiple jobs writing to a table in your lake, duplicate rows can be generated, even when you include this option. Values: ( {COLUMNS = (, ...) | COLUMN = }, WINDOW = { MINUTE[S] | HOUR[S] | DAY[S] } ) |
| end_at | s3 | job_options | value | True | True | Configures the time to stop ingesting data. Files after the specified time are ignored. Timestamps should be based on UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS' Values: { NOW | <timestamp> } Default: Never |
| start_from | s3 | job_options | value | False | True | Configures the time to start ingesting data. Files before the specified time are ignored. Timestamps should be in the UTC time format. When a DATE_PATTERN is not specified, configuring this option is not allowed. By default, all available data is ingested. If the DATE_PATTERN is not lexicographically ordered, then this option cannot be set to BEGINNING. Values: { NOW | BEGINNING | <timestamp> } Default: BEGINNING |
| compute_cluster | s3 | job_options | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is only one cluster in your environment. If you have more than one compute cluster, you need to determine which one to use through this option. Default: The sole cluster in your environment |
| run_parallelism | s3 | job_options | integer | True | True | The number of parser jobs to run in parallel per minute. Default: 1 |
| content_type | s3 | job_options | value | True | True | Values: { AUTO | CSV | JSON | PARQUET | TSV | AVRO | AVRO_SCHEMA_REGISTRY | FIXED_WIDTH | REGEX | SPLIT_LINES | ORC | XML } Default: AUTO The file format of the content being read. Note that AUTO only works when reading Avro, JSON, or Parquet. |
| compression | s3 | job_options | value | False | True | The compression of the source. Default: AUTO. |
| comment | s3 | job_options | text | True | True | A description or comment regarding this job. |
| column_transformations | s3 | job_options | dict | False | True | If transformations must be applied prior to data landing in your target, you can use this option to perform data transformations during ingestion. When ingesting into the data lake, it is recommended that you only apply essential transformations, such as protecting PII, as it is easier to make amendments or corrections at a later date if the data remains in its raw state and instead use a transformation job to apply modifications. Therefore, as a general rule, you should only transform data that must be modified before it reaches the target. However, transformations provide the flexibility to shape your data before it lands in the target. You can use all the functions and operators supported by Upsolver to create calculated fields within your ingestion job. New columns can be added to your target, and existing column data can be transformed. You can perform actions such as converting data types, formatting string values, and concatenating columns to create a new column. If you need to mask sensitive or personally identifiable information (PII) prior to loading into your staging tables or when performing direct ingestion into your target destination, you can use hashing functions to prevent data from being exposed downstream. Combining hash functions with the EXCLUDE_COLUMNS option enables you to control your data protection. |
| exclude_columns | s3 | job_options | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. |
| stream | kinesis | source_options | text | False | False | The stream to read from. |
| reader_shards | kinesis | job_options | integer | True | True | Determines how many readers are used in parallel to read the stream. This number does not need to equal your number of shards in Kinesis. A recommended value would be to increase it by 1 for every 70 MB/s sent to your stream. Default: 1 |
| store_raw_data | kinesis | job_options | boolean | False | True | When true, an additional copy of the data is stored in its original format. Default: false |
| start_from | kinesis | job_options | value | False | True | Configures the time to start ingesting data. Files dated before the specified time are ignored. Values: { NOW | BEGINNING } Default: BEGINNING |
| end_at | kinesis | job_options | value | False | True | Configures the time to stop ingesting data. Files after the specified time are ignored. Timestamps should be based on UTC and in the following format: TIMESTAMP 'YYYY-MM-DD HH:MM:SS' Values: { NOW | <timestamp> } Default: Never |
| compute_cluster | kinesis | job_options | identifier | True | True | The compute cluster to run this job. This option can only be omitted when there is only one cluster in your environment. If you have more than one compute cluster, you need to determine which one to use through this option. Default: The sole cluster in your environment |
| run_parallelism | kinesis | job_options | integer | False | True | The number of parser jobs to run in parallel per minute. Default: 1 |
| content_type | kinesis | job_options | value | True | True | The file format of the content being read. Note that AUTO only works when reading Avro, JSON, or Parquet. To configure additional options for certain content types, see Content type options. Values: { AUTO | CSV | JSON | PARQUET | TSV | AVRO | AVRO_SCHEMA_REGISTRY | FIXED_WIDTH | REGEX | SPLIT_LINES | ORC | XML } Default: AUTO |
| compression | kinesis | job_options | value | False | True | The compression of the source. Values: { AUTO | GZIP | SNAPPY | LZO | NONE | SNAPPY_UNFRAMED | KCL } Default: AUTO |
| comment | kinesis | job_options | text | True | True | A description or comment regarding this job. |
| column_transformations | kinesis | job_options | text | True | True | If transformations must be applied prior to data landing in your target, you can use this option to perform data transformations during ingestion. When ingesting into the data lake, it is recommended that you only apply essential transformations, such as protecting PII, as it is easier to make amendments or corrections at a later date if the data remains in its raw state and instead use a transformation job to apply modifications. Therefore, as a general rule, you should only transform data that must be modified before it reaches the target. However, transformations provide the flexibility to shape your data before it lands in the target. You can use all the functions and operators supported by Upsolver to create calculated fields within your ingestion job. New columns can be added to your target, and existing column data can be transformed. You can perform actions such as converting data types, formatting string values, and concatenating columns to create a new column. If you need to mask sensitive or personally identifiable information (PII) prior to loading into your staging tables or when performing direct ingestion into your target destination, you can use hashing functions to prevent data from being exposed downstream. Combining hash functions with the EXCLUDE_COLUMNS option enables you to control your data protection. |
| deduplicate_with | kinesis | job_options | values | False | True | You can use DEDUPLICATE_WITH to prevent duplicate rows arriving in your target. One or more columns can be supplied in the column list to act as a key so that all events within the timeframe specified in the WINDOW value are deduplicated. For example, if you have a third-party application that sends the same event multiple times a day, you can define one or more columns as the key and set the timeframe to be 1 DAY. Upsolver will exclude all duplicate events that arrive within the day, ensuring your target only receives unique events. Note that if you have multiple jobs writing to a table in your lake, duplicate rows can be generated, even when you include this option. Values: ( {COLUMNS = (, ...) | COLUMN = }, WINDOW = { MINUTE[S] | HOUR[S] | DAY[S] } ) |
| exclude_columns | kinesis | job_options | list | False | True | The EXCLUDE_COLUMNS option tells Upsolver to ignore data in the columns specified in this list, and the column is not created on the target. To exclude columns, provide a single column or a list of column names, or use a glob pattern. When you simply don't need columns, you want to save storage space, or maintain a clean data structure, use EXCLUDE_COLUMNS and the specified columns will be ignored. This option gives you control over the width of the target table by enabling you to manage how many columns are created. If your target system has a limit on the number of columns it supports, continuously adding columns can cause issues. Furthermore, columns containing sensitive information can be excluded, ensuring private data is not copied downstream to a staging table in your data lake, or directly into your target. |
